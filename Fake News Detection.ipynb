{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94adbfd8-5098-4c23-acdc-9dadae142531",
   "metadata": {},
   "source": [
    "# NLP Project Fake News\n",
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5594a0f8-3b1c-437f-a4a6-f2d9eaf6c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Daniel Greenfield, a Shillman Journalism Fello...\n",
      "1    Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
      "2    U.S. Secretary of State John F. Kerry said Mon...\n",
      "3    â€” Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
      "4    It's primary day in New York and front-runners...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# We Will use 2 Column\n",
    "df = pd.read_csv('news.csv', usecols=['text','label']) \n",
    "\n",
    "# Drop rows with any empty cells\n",
    "df.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=None,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Encoding Label Column\n",
    "df['class'] = np.where(df['label']=='FAKE',0,1)\n",
    "\n",
    "#Removing Duplicate\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#Test Data\n",
    "print(df['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598fd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization --> Expand Contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "} \n",
    "\n",
    "# Tokenization Regular expression To find contractions\n",
    "contractions_RE = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "def ExpandContractions(text,contractions = contractions):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_RE.sub(replace, text)\n",
    "\n",
    "# Applay Expanding Contractions\n",
    "df['text'] = df['text'].apply(lambda x:ExpandContractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbd0f05-9e64-412e-94cb-62ae48b7b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df['text'] = df['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5357bae0-1473-4048-888a-30260f13c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5019d3d-626d-4b8c-b4e1-b55c7b35e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Numbers --> remove words containing digits\n",
    "df['text'] = df['text'].apply(lambda x: re.sub('W*dw*','',x))\n",
    "\n",
    "# Replace Any thing not Digite or char with spaces\n",
    "df['text'] = (df['text'].str.replace('[^A-Za-z0-9\\s]','', regex=True).str.replace('\\n','', regex=True).str.replace('\\s+',' ', regex=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03e7dd1-271d-4811-b41a-4de882d5f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b52a72-bdbc-4d3c-9c70-b3ff2942f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrase text ==> url\n",
    "df['text'] = df['text'].apply(lambda x:re.sub('(http[s]?S+)|(w+.[A-Za-z]{2,4}S*)', 'urladd', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c35953-2826-4680-8ff3-da57afc9aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: stem_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bfe263c-bc1f-4f1e-8370-2d50b11a5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63562f32-1d95-40d7-9074-b97013d09ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    aniel greenfiel shillman journal fellourladdom...\n",
      "1    googl pinterest igg linkein reit stumbleupon p...\n",
      "2    u secretari state john f kerri sai monay stop ...\n",
      "3    kaye king kayeek novemb 9 2016 lesson tonight ...\n",
      "4    primari ay neurladd frontrunn hillari clinton ...\n",
      "Name: text, dtype: object\n",
      "aniel greenfiel shillman journal fellourladdom center neurladd urladd focus raical islam final stretch elect hillari roham clinton gone war fbi wor unpreceent thrown aroun often elect ought retir still unpreceent nomine major polit parti go war fbi that exactli hillari peopl one coma patient urladd urladdng hour cnn hospit be urladd assum fbi irector jame comey hillari oppon elect fbi uner attack everyon obama cnn hillari peopl circul letter attack comey current meia hit piec lambast target trump urladd surpris clinton alli start run attack fbi fbi leaership urladd entir lefturladd establish form lynch mob continu go hillari fbi creibil attack meia emocrat preemptiv hea result investig clinton founat hillari clinton covert struggl fbi agent obama oj peopl gone explos public neurladd time compar comey j egar hoover bizarr healin jame comey role recal hoover fbi fairli practic amit front spout nonsens boston globe publish column call comey resign outon time eitori claim scanal realli attack urladd jame carvil appear msnbc remin everyon still aliv insan accus comey coorin hous republican kgb thought vast right urladd conspiraci stretch countless meia stori charg comey violat proceur knourladd proceur violat email classifi inform store bathroom server senat harri rei sent comey letter accus violat hatch act hatch act nice iea much relev age obama tenth amen cabl news spectrum quickli fill meia hack glanc urladdia articl hatch act uner tabl accus fbi irector one aurladd conspiraci hillari ever jame comey realli hurt hillari pick one hell strang way long ago emocrat breath sigh relief gave hillari clinton pas promin public statement realli elect trump keep email scanal go trash investig payrol hous republican kgb back play coy suen evelop vlaimir putin paul ryan talk take look anthoni urladd comput either comey cun fbi irector ever live he aurladdi tri navig polit mess trapp oj leaership urladd polit futur tie hillari victori bureau urladd apolit agent urladd allow job truli mysteri thing hillari associ eci go war respect feeral agenc american like fbi hillari clinton enjoy 60 unfavor rate interest question hillari ol strategi lie eni fbi even ha crimin investig unerway instea associ insist secur revieurladd correct shrugg ol breezi enial approach given way savag assault fbi preten noth urladd ba strategi better one pick fight fbi lunat clinton associ tri claim fbi realli kgb two possibl explan hillari clinton might arrog enough lash fbi believ victori near kin hubri le plan victori fireurladd isplay coul lea eclar war fbi irrit ure final mile campaign explan peopl panick go war fbi behavior smart focus presienti campaign act esper presienti caniat eci option tri estroy creibil fbi that hubri fear fbi might reveal ure origin fbi investig hillari clinton confient coul rie ha goo reason believ hillari clinton gone place paranoi urladd urladd short space time posit clinton campaign promis unit countri replac esper flail oper focus energi fight fbi there one reason bizarr behavior clinton campaign eci fbi investig latest batch email pose threat surviv gone fight fbi unpreceent step born fear har knourladdh fear justifi exist fear alreay tell u urladd lot clinton loyalist rigg ol investig kneurladdom ahea time urladd kneurladd question suenli longer control afrai smell fear fbi urladdp investig clinton founat fine neurladdl time clintonurladd panick spinmeist clintonurladd claim email scanal much smoke urladdt fire that appear improprieti urladdt substanc isnt react smoke respon fire misgui assault fbi tell u hillari clinton alli afrai revel bigger funament illeg email setup email setup preemptiv cover clinton campaign panick bali belief right urladd urladd crime illeg setup meant cover risk expo clinton urladdr countless scanal year urladd protect time aroun bigger usual corrupt briberi sexual assault abus power follow aroun throughout year bigger amag alleg alreay come ont urladd fbi investig anyurladd near campaign comey pure intimi also urladdg senior fbi peopl valu career urladd stay away emocrat close rank aroun nomine fbi ugli unpreceent scene may also last stan hillari clinton aurladdi urladd way numer scanal elect cycl she never shown fear esper chang urladd afrai lie buri email huma abein bring like noth el\n"
     ]
    }
   ],
   "source": [
    "# Removing Extra Spaces\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: re.sub(' +', ' ', text))\n",
    "\n",
    "print(df[\"text\"].head())\n",
    "print(df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55e613f-2dfe-47c8-b16f-149d4e49e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    FAKE\n",
      "1    FAKE\n",
      "2    REAL\n",
      "3    FAKE\n",
      "4    REAL\n",
      "Name: label, dtype: object\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: class, dtype: int32\n",
      "                                                   text  class\n",
      "0     aniel greenfiel shillman journal fellourladdom...      0\n",
      "1     googl pinterest igg linkein reit stumbleupon p...      0\n",
      "2     u secretari state john f kerri sai monay stop ...      1\n",
      "3     kaye king kayeek novemb 9 2016 lesson tonight ...      0\n",
      "4     primari ay neurladd frontrunn hillari clinton ...      1\n",
      "...                                                 ...    ...\n",
      "6330  state epart tol republican nation committe cou...      1\n",
      "6331  p pb shoul stan plutocrat pentagon post oct 27...      0\n",
      "6332  antitrump protest tool oligarchi reform alurla...      0\n",
      "6333  ai ababa ethiopia presient obama conven meet l...      1\n",
      "6334  jeb bush suenli attack trump here mattersjeb b...      1\n",
      "\n",
      "[6060 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].head())\n",
    "print(df[\"class\"].head())\n",
    "\n",
    "#Removing Label Column\n",
    "df = df.drop('label', axis=1)\n",
    "print(df)\n",
    "\n",
    "#Saving New CSV File\n",
    "df.to_csv('new_news', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
